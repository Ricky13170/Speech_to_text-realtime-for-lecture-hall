import numpy as np
import logging
from dataclasses import replace as dataclass_replace
from backend.audio_normalizer import AdaptiveNormalizer
from backend.torch_patch import apply_torch_load_patch, suppress_stdout

logger = logging.getLogger(__name__)

SAMPLE_RATE = 16000


class WhisperASR:
    def __init__(self, model_size="large-v3", language=None, device="cuda", cache_dir=None):
        self.model_size = model_size
        self.language = language
        self.device = device
        self.cache_dir = cache_dir
        self.model = None
        
        self.normalizer = AdaptiveNormalizer(target_dB=-20.0, sample_rate=SAMPLE_RATE)
        logger.info("[ASR] WhisperX with adaptive audio normalization enabled")

    def load_model(self):
        if self.model is not None:
            return
        
        # Apply PyTorch patches for pyannote compatibility
        apply_torch_load_patch()
        
        import whisperx
        
        logger.info(f"Loading WhisperX: {self.model_size}")
        
        compute_type = "float16" if self.device == "cuda" else "int8"
        
        with suppress_stdout():
            self.model = whisperx.load_model(
                self.model_size,
                self.device,
                compute_type=compute_type,
                language=self.language,
                asr_options={"without_timestamps": True},
                # Disable internal Pyannote VAD — already handled at streaming level
                # (WhisperX requires vad_onset > 0, so use near-zero threshold)
                vad_options={"vad_onset": 0.01, "vad_offset": 0.01},
            )
        
        logger.info("WhisperX loaded")

    def transcribe(self, audio: np.ndarray, prompt: str = None) -> tuple:
        """Transcribe audio using WhisperX.
        
        Args:
            audio: float32 audio at 16kHz
            prompt: Context prompt (recent transcript history)
            
        Returns:
            (text, confidence) — same interface as before
        """
        if self.model is None:
            self.load_model()
        
        if len(audio) < 8000:
            return "", 0.0
        
        normalized_audio, norm_stats = self.normalizer.normalize(audio)

        if normalized_audio.dtype != np.float32:
            normalized_audio = normalized_audio.astype(np.float32)
        
        # Set initial_prompt via model options
        # (WhisperX uses TranscriptionOptions, NOT a transcribe() kwarg)
        original_options = None
        if prompt and hasattr(self.model, 'options'):
            original_options = self.model.options
            self.model.options = dataclass_replace(
                self.model.options, initial_prompt=prompt
            )
        
        try:
            # Transcribe with WhisperX (batch_size=1 for streaming segments)
            result = self.model.transcribe(
                normalized_audio,
                batch_size=1,
                language=self.language,
            )
        finally:
            # Restore original options (model is shared across sessions)
            if original_options is not None:
                self.model.options = original_options
        
        # Collect segments and calculate confidence
        text_parts = []
        total_prob = 0.0
        seg_count = 0
        
        for segment in result.get("segments", []):
            text_parts.append(segment.get("text", "").strip())
            if "avg_logprob" in segment:
                total_prob += segment["avg_logprob"]
                seg_count += 1
        
        text = " ".join(text_parts).strip()
        
        # Calculate confidence
        if seg_count > 0:
            avg_logprob = total_prob / seg_count
            conf = min(1.0, max(0.0, 1.0 + avg_logprob / 2))
        elif text:
            # WhisperX may not return avg_logprob — default to reasonable confidence
            conf = 0.8
        else:
            conf = 0.0
        
        return text, conf


class StreamingASRProcessor:
    """Streaming processor for audio chunks"""
    
    def __init__(self, asr: WhisperASR, context_size: int = 3):
        self.asr = asr
        self.context_size = context_size
        self.audio_buffer = np.array([], dtype=np.float32)
        self.context_history = []

    def init(self):
        self.audio_buffer = np.array([], dtype=np.float32)
        self.context_history = []

    def insert_audio(self, audio: np.ndarray):
        self.audio_buffer = np.concatenate([self.audio_buffer, audio])

    def process_partial(self, prompt: str = None) -> tuple:
        if len(self.audio_buffer) < SAMPLE_RATE:
            return "", 0.0
        
        # Whisper will normalize internally
        text, conf = self.asr.transcribe(self.audio_buffer, prompt)
        return text, conf

    def process_final(self, prompt: str = None) -> tuple:
        if len(self.audio_buffer) < 4000:
            self.audio_buffer = np.array([], dtype=np.float32)
            return "", 0.0
        
        # Whisper will normalize internally
        text, conf = self.asr.transcribe(self.audio_buffer, prompt)
        
        if text:
            self.context_history.append(text)
            if len(self.context_history) > self.context_size * 2:
                self.context_history = self.context_history[-self.context_size:]
        
        self.audio_buffer = np.array([], dtype=np.float32)
        return text, conf
